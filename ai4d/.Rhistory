knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
library(dplyr)
train <- read.csv("Train.csv")
test <- read.csv("Test.csv")
library(skimr)
skimr::skim(train)
train[train==""]<- NA
test[test==""]<- NA
library(visdat)
visdat::vis_dat(train)
Train <- train%>%
mutate(total = train$total_female+train$total_male)
Test <- test%>%
mutate(total = test$total_female+test$total_male)
#x <- Train$travel_with
#for (i in x) {
#  if (is.na(x[i]) & Train$total == 1){
#    return("Alone")
#  }
#}
Test <- na.omit(Test)
Train<- na.omit(Train)
Train[!(Train$total ==0),]
Test[!(Test$total ==0),]
cat_col_Train <- c( 'country', 'age_group', 'travel_with', 'purpose',
'main_activity', 'info_source', 'tour_arrangement',
'package_transport_int', 'package_accomodation', 'package_food',
'package_transport_tz', 'package_sightseeing', 'package_guided_tour',
'package_insurance', 'first_trip_tz', 'cost_category')
cat_col_Test <- c( 'country', 'age_group', 'travel_with', 'purpose',
'main_activity', 'info_source', 'tour_arrangement',
'package_transport_int', 'package_accomodation', 'package_food',
'package_transport_tz', 'package_sightseeing', 'package_guided_tour',
'package_insurance', 'first_trip_tz')
library(CatEncoders)
cat_data_Train <- Train[cat_col_Train]
lenc <- sapply(Train[cat_col_Train],function(x) LabelEncoder.fit(x))
for (i in cat_col_Train){
cat_data_Train[[i]] <- transform(lenc[[i]],Train[[i]])
}
cat_data_Test <- Test[cat_col_Test]
lenc <- sapply(Test[cat_col_Test],function(x) LabelEncoder.fit(x))
for (i in cat_col_Test){
cat_data_Test[[i]] <- transform(lenc[[i]],Test[[i]])
}
cat_data_Train
library(ggplot2)
options(repr.plot.width=4, repr.plot.height=4)
ggplot(cat_data_Train, aes(x=cost_category))+geom_bar(fill="blue",alpha=0.5)+theme_bw()+labs(title="Distribution of Cost Category")
df_Train <- cbind(cat_data_Train,Train[c('total_female', 'total_male','night_mainland','night_zanzibar')])
df_Test <- cbind(cat_data_Test,Test[c('total_female', 'total_male','night_mainland','night_zanzibar')])
library(corrplot)
options(repr.plot.width=4, repr.plot.height=4)
correlationMatrix <- cor(df_Train[,3:ncol(df_Train)])
corrplot(correlationMatrix, order = "hclust", tl.cex = 1, addrect = 8)
reticulate::repl_python()
import os
import pandas as pd
from pandas import DataFrame,Series
from sklearn import tree
import matplotlib
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm
from sklearn.preprocessing import StandardScaler
import statsmodels.formula.api as smf
import statsmodels.api as sm
from mpl_toolkits.mplot3d import Axes3D
import seaborn as sns
from sklearn import neighbors
from sklearn import linear_model
quit
X <- df_Train%>%select(-cost_category)  #independent columns without the index column
y <- df_Train$cost_category    #target column i.e price range
reticulate::repl_python()
X_train = r.X
y_test = r.y
Test = r.df_Test
Test['target'] = "Test"
X_test = Test.iloc[:,1:20]
y_test = Test['target']
X_test.shape,X_train.shape
# GETTING Correllation matrix
corr_mat=X_train.corr(method='pearson')
plt.figure(figsize=(20,10))
sns.heatmap(corr_mat,vmax=1,square=True,annot=True,cmap='cubehelix')
from sklearn.preprocessing import StandardScaler
X_train=X_train.values
X_train=np.asarray(X_train)
# Finding normalised array of X_Train
XTrain = StandardScaler().fit_transform(X_train)
from sklearn.decomposition import PCA
pca = PCA().fit(XTrain)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
plt.xlim(0,7,1)
plt.xlabel('Number of components')
plt.ylabel('Cumulative explained variance')
quit
set.seed(1815)
train_data <- py$XTrain
test_data <-  py$X_test
train_y  <-  y
teat_y   <-  py$y_test
library(caret)
fitControl <- trainControl(method="cv", #Control the computational nuances of the train function
number = 15, #Either the number of folds or number of resampling iterations
classProbs = TRUE,
summaryFunction = twoClassSummary)
model_naiveb <- train(diagnosis~.,
train_data,
method="nb",
metric="ROC",
preProcess=c('center', 'scale'), #in order to normalize the data
trace=FALSE,
trControl=fitControl)
library(caret)
fitControl <- trainControl(method="cv", #Control the computational nuances of the train function
number = 15, #Either the number of folds or number of resampling iterations
classProbs = TRUE,
summaryFunction = twoClassSummary)
model_naiveb <- train(cost_category~.,
train_data,
method="nb",
metric="ROC",
preProcess=c('center', 'scale'), #in order to normalize the data
trace=FALSE,
trControl=fitControl)
library(caret)
fitControl <- trainControl(method="cv", #Control the computational nuances of the train function
number = 15, #Either the number of folds or number of resampling iterations
classProbs = TRUE,
summaryFunction = twoClassSummary)
model_naiveb <- train(train_y~.,
train_data,
method="nb",
metric="ROC",
preProcess=c('center', 'scale'), #in order to normalize the data
trace=FALSE,
trControl=fitControl)
library(caret)
fitControl <- trainControl(method="cv", #Control the computational nuances of the train function
number = 15, #Either the number of folds or number of resampling iterations
classProbs = TRUE,
summaryFunction = twoClassSummary)
model_naiveb <- train(cost_category~.,
df_Train,
method="nb",
metric="ROC",
preProcess=c('center', 'scale'), #in order to normalize the data
trace=FALSE,
trControl=fitControl)
reticulate::repl_python()
best_model, best_model_name, acc = best_model.bestClassificationModel(X, Y)
print(best_model)
print(best_model_name, ":", acc)
View(Test)
setwd("C:/Users/Guy/Desktop/Git/Learn/Breast-Cancer-Prediction-Project-master/Breast-Cancer-Prediction-Project-master")
knitr::opts_chunk$set(comment=NA, message=FALSE, warning=FALSE)
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(corrplot)) install.packages("recorrplotadr", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(pROC)) install.packages("pROC", repos = "http://cran.us.r-project.org")
if(!require(caTools)) install.packages("caTools", repos = "http://cran.us.r-project.org")
if(!require(caretEnsemble)) install.packages("caretEnsemble", repos = "http://cran.us.r-project.org")
if(!require(grid)) install.packages("grid", repos = "http://cran.us.r-project.org")
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(ggfortify)) install.packages("ggfortify", repos = "http://cran.us.r-project.org")
if(!require(glmnet)) install.packages("glmnet", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(nnet)) install.packages("nnet", repos = "http://cran.us.r-project.org")
if(!require(funModeling)) install.packages("funModeling", repos = "http://cran.us.r-project.org")
if(!require(Momocs)) install.packages("Momocs", repos = "http://cran.us.r-project.org")
library(funModeling)
library(corrplot)
# The data file will be loaded from my personal github account
data <- read.csv("https://raw.githubusercontent.com/gmineo/Breast-Cancer-Prediction/master/data.csv")
knitr::opts_chunk$set(comment=NA, message=FALSE, warning=FALSE)
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")
if(!require(ggplot2)) install.packages("ggplot2", repos = "http://cran.us.r-project.org")
if(!require(corrplot)) install.packages("recorrplotadr", repos = "http://cran.us.r-project.org")
if(!require(gridExtra)) install.packages("gridExtra", repos = "http://cran.us.r-project.org")
if(!require(pROC)) install.packages("pROC", repos = "http://cran.us.r-project.org")
if(!require(caTools)) install.packages("caTools", repos = "http://cran.us.r-project.org")
if(!require(caretEnsemble)) install.packages("caretEnsemble", repos = "http://cran.us.r-project.org")
if(!require(grid)) install.packages("grid", repos = "http://cran.us.r-project.org")
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(ggfortify)) install.packages("ggfortify", repos = "http://cran.us.r-project.org")
if(!require(glmnet)) install.packages("glmnet", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(nnet)) install.packages("nnet", repos = "http://cran.us.r-project.org")
if(!require(funModeling)) install.packages("funModeling", repos = "http://cran.us.r-project.org")
if(!require(Momocs)) install.packages("Momocs", repos = "http://cran.us.r-project.org")
library(funModeling)
library(corrplot)
# The data file will be loaded from my personal github account
data <- read.csv("data.csv")
data$diagnosis <- as.factor(data$diagnosis)
# the 33 column is invalid
data[,33] <- NULL
str(data)
head(data)
summary(data)
map(data, function(.x) sum(is.na(.x)))
prop.table(table(data$diagnosis))
options(repr.plot.width=4, repr.plot.height=4)
ggplot(data, aes(x=diagnosis))+geom_bar(fill="blue",alpha=0.5)+theme_bw()+labs(title="Distribution of Diagnosis")
plot_num(data %>% select(-id), bins=10)
correlationMatrix <- cor(data[,3:ncol(data)])
corrplot(correlationMatrix, order = "hclust", tl.cex = 1, addrect = 8)
# find attributes that are highly corrected (ideally >0.90)
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.9)
# print indexes of highly correlated attributes
print(highlyCorrelated)
# Remove correlated variables
data2 <- data %>%select(-highlyCorrelated)
# number of columns after removing correlated variables
ncol(data2)
pca_res_data <- prcomp(data[,3:ncol(data)], center = TRUE, scale = TRUE)
plot(pca_res_data, type="l")
summary(pca_res_data)
pca_res_data2 <- prcomp(data2[,3:ncol(data2)], center = TRUE, scale = TRUE)
plot(pca_res_data2, type="l")
summary(pca_res_data2)
pca_df <- as.data.frame(pca_res_data2$x)
ggplot(pca_df, aes(x=PC1, y=PC2, col=data$diagnosis)) + geom_point(alpha=0.5)
g_pc1 <- ggplot(pca_df, aes(x=PC1, fill=data$diagnosis)) + geom_density(alpha=0.25)
g_pc2 <- ggplot(pca_df, aes(x=PC2, fill=data$diagnosis)) + geom_density(alpha=0.25)
grid.arrange(g_pc1, g_pc2, ncol=2)
lda_res_data <- MASS::lda(diagnosis~., data = data, center = TRUE, scale = TRUE)
lda_res_data
#Data frame of the LDA for visualization purposes
lda_df_predict <- predict(lda_res_data, data)$x %>% as.data.frame() %>% cbind(diagnosis=data$diagnosis)
ggplot(lda_df_predict, aes(x=LD1, fill=diagnosis)) + geom_density(alpha=0.5)
set.seed(1815)
data3 <- cbind (diagnosis=data$diagnosis, data2)
data_sampling_index <- createDataPartition(data$diagnosis, times=1, p=0.8, list = FALSE)
train_data <- data3[data_sampling_index, ]
test_data <- data3[-data_sampling_index, ]
fitControl <- trainControl(method="cv",    #Control the computational nuances of thetrainfunction
number = 15,    #Either the number of folds or number of resampling iterations
classProbs = TRUE,
summaryFunction = twoClassSummary)
model_naiveb <- train(diagnosis~.,
train_data,
method="nb",
metric="ROC",
preProcess=c('center', 'scale'), #in order to normalize the data
trace=FALSE,
trControl=fitControl)
prediction_naiveb <- predict(model_naiveb, test_data)
confusionmatrix_naiveb <- confusionMatrix(prediction_naiveb, test_data$diagnosis, positive = "M")
confusionmatrix_naiveb
plot(varImp(model_naiveb), top=10, main="Top variables- Naive Bayes")
model_logreg<- train(diagnosis ~., data = train_data, method = "glm",
metric = "ROC",
preProcess = c("scale", "center"),  # in order to normalize the data
trControl= fitControl)
prediction_logreg<- predict(model_logreg, test_data)
# Check results
confusionmatrix_logreg <- confusionMatrix(prediction_logreg, test_data$diagnosis, positive = "M")
confusionmatrix_logreg
plot(varImp(model_logreg), top=10, main="Top variables - Log Regr")
model_randomforest <- train(diagnosis~.,
train_data,
method="rf",  #also recommended ranger, because it is a lot faster than original randomForest (rf)
metric="ROC",
#tuneLength=10,
#tuneGrid = expand.grid(mtry = c(2, 3, 6)),
preProcess = c('center', 'scale'),
trControl=fitControl)
prediction_randomforest <- predict(model_randomforest, test_data)
#Check results
confusionmatrix_randomforest <- confusionMatrix(prediction_randomforest, test_data$diagnosis, positive = "M")
confusionmatrix_randomforest
plot(varImp(model_randomforest), top=10, main="Top variables- Random Forest")
model_knn <- train(diagnosis~.,
train_data,
method="knn",
metric="ROC",
preProcess = c('center', 'scale'),
tuneLength=10, #The tuneLength parameter tells the algorithm to try different default values for the main parameter
#In this case we used 10 default values
trControl=fitControl)
prediction_knn <- predict(model_knn, test_data)
confusionmatrix_knn <- confusionMatrix(prediction_knn, test_data$diagnosis, positive = "M")
confusionmatrix_knn
plot(varImp(model_knn), top=10, main="Top variables - KNN")
model_nnet_pca <- train(diagnosis~.,
train_data,
method="nnet",
metric="ROC",
preProcess=c('center', 'scale', 'pca'),
tuneLength=10,
trace=FALSE,
trControl=fitControl)
View(data)
View(data)
View(data)
View(data)
View(data)
prediction_nnet_pca <- predict(model_nnet_pca, test_data)
confusionmatrix_nnet_pca <- confusionMatrix(prediction_nnet_pca, test_data$diagnosis, positive = "M")
confusionmatrix_nnet_pca
plot(varImp(model_nnet_pca), top=8, main="Top variables - NNET PCA")
train_data_lda <- lda_df_predict[data_sampling_index, ]
test_data_lda <- lda_df_predict[-data_sampling_index, ]
model_nnet_lda <- train(diagnosis~.,
train_data_lda,
method="nnet",
metric="ROC",
preProcess=c('center', 'scale'),
tuneLength=10,
trace=FALSE,
trControl=fitControl)
gc()
prediction_nnet_lda <- predict(model_nnet_lda, test_data_lda)
confusionmatrix_nnet_lda <- confusionMatrix(prediction_nnet_lda, test_data_lda$diagnosis, positive = "M")
confusionmatrix_nnet_lda
models_list <- list(Naive_Bayes=model_naiveb,
Logistic_regr=model_logreg,
Random_Forest=model_randomforest,
KNN=model_knn,
Neural_PCA=model_nnet_pca,
Neural_LDA=model_nnet_lda)
models_results <- resamples(models_list)
summary(models_results)
bwplot(models_results, metric="ROC")
confusionmatrix_list <- list(
Naive_Bayes=confusionmatrix_naiveb,
Logistic_regr=confusionmatrix_logreg,
Random_Forest=confusionmatrix_randomforest,
KNN=confusionmatrix_knn,
Neural_PCA=confusionmatrix_nnet_pca,
Neural_LDA=confusionmatrix_nnet_lda)
confusionmatrix_list_results <- sapply(confusionmatrix_list, function(x) x$byClass)
confusionmatrix_list_results %>% knitr::kable()
confusionmatrix_results_max <- apply(confusionmatrix_list_results, 1, which.is.max)
output_report <- data.frame(metric=names(confusionmatrix_results_max),
best_model=colnames(confusionmatrix_list_results)[confusionmatrix_results_max],
value=mapply(function(x,y) {confusionmatrix_list_results[x,y]},
names(confusionmatrix_results_max),
confusionmatrix_results_max))
rownames(output_report) <- NULL
output_report
print("Operating System:")
version
setwd("~/GitHub/AI4D-Africa-s-Anglophone-Research-Lab-Tanzania-Tourism-Classification/ai4d")
knitr::opts_chunk$set(echo = TRUE)
library(reticulate)
library(dplyr)
train <- read.csv("Train.csv")
test <- read.csv("Test.csv")
library(visdat)
train[train==""]<- NA
test[test==""]<- NA
visdat::vis_dat(train)
reticulate::repl_python()
Train = r.train
